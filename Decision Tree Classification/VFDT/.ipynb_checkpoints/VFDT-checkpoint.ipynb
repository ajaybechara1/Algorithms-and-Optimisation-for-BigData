{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and tst data into 80% and 20%\n",
    "# We drop the index if already exist and re-assign from 0 \n",
    "\n",
    "def train_test_split(dataset):\n",
    "    trainLen = int(len(dataset)*0.8)\n",
    "    training_data = dataset.iloc[:trainLen].reset_index(drop=True) \n",
    "    testing_data = dataset.iloc[trainLen:].reset_index(drop=True)\n",
    "    return training_data,testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTitanicData(train,test):\n",
    "    #Copy original dataset in case we need it later when digging into interesting features\n",
    "    # WARNING: Beware of actually copying the dataframe instead of just referencing it\n",
    "    # \"original_train = train\" will create a reference to the train variable (changes in 'train' will apply to 'original_train')\n",
    "    original_train = train.copy() # Using 'copy()' allows to clone the dataset, creating a different object with the same values\n",
    "\n",
    "    # Feature engineering steps taken from Sina and Anisotropic, with minor changes to avoid warnings\n",
    "    full_data = [train, test]\n",
    "\n",
    "    # Feature that tells whether a passenger had a cabin on the Titanic\n",
    "    train['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "    test['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "\n",
    "    # Create new feature FamilySize as a combination of SibSp and Parch\n",
    "    for dataset in full_data:\n",
    "        dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "    # Create new feature IsAlone from FamilySize\n",
    "    for dataset in full_data:\n",
    "        dataset['IsAlone'] = 0\n",
    "        dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "    # Remove all NULLS in the Embarked column\n",
    "    for dataset in full_data:\n",
    "        dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
    "    # Remove all NULLS in the Fare column\n",
    "    for dataset in full_data:\n",
    "        dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n",
    "\n",
    "    # Remove all NULLS in the Age column\n",
    "    for dataset in full_data:\n",
    "        age_avg = dataset['Age'].mean()\n",
    "        age_std = dataset['Age'].std()\n",
    "        age_null_count = dataset['Age'].isnull().sum()\n",
    "        age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n",
    "        # Next line has been improved to avoid warning\n",
    "        dataset.loc[np.isnan(dataset['Age']), 'Age'] = age_null_random_list\n",
    "        dataset['Age'] = dataset['Age'].astype(int)\n",
    "\n",
    "    # Define function to extract titles from passenger names\n",
    "    def get_title(name):\n",
    "        title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "        # If the title exists, extract and return it.\n",
    "        if title_search:\n",
    "            return title_search.group(1)\n",
    "        return \"\"\n",
    "\n",
    "    for dataset in full_data:\n",
    "        dataset['Title'] = dataset['Name'].apply(get_title)\n",
    "    # Group all non-common titles into one single grouping \"Rare\"\n",
    "    for dataset in full_data:\n",
    "        dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "        dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "        dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "        dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "    for dataset in full_data:\n",
    "        # Mapping Sex\n",
    "        dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "\n",
    "        # Mapping titles\n",
    "        title_mapping = {\"Mr\": 1, \"Master\": 2, \"Mrs\": 3, \"Miss\": 4, \"Rare\": 5}\n",
    "        dataset['Title'] = dataset['Title'].map(title_mapping)\n",
    "        dataset['Title'] = dataset['Title'].fillna(0)\n",
    "\n",
    "        # Mapping Embarked\n",
    "        dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
    "\n",
    "        # Mapping Fare\n",
    "        dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n",
    "        dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
    "        dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
    "        dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n",
    "        dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "\n",
    "        # Mapping Age\n",
    "        dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n",
    "        dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
    "        dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
    "        dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
    "        dataset.loc[ dataset['Age'] > 64, 'Age'] ;\n",
    "\n",
    "    # Feature selection: remove variables no longer containing relevant information\n",
    "    drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\n",
    "    train = train.drop(drop_elements, axis = 1)\n",
    "    test = test.drop(drop_elements, axis = 1)\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatData(t,s):\n",
    "    if not isinstance(t,dict) and not isinstance(t,list):\n",
    "        print(str(\"\\t\"*s)+str(t))\n",
    "    else:\n",
    "        for key in t:\n",
    "            print(\"\\t\"*s+str(key))\n",
    "            if not isinstance(t,list):\n",
    "                formatData(t[key],s+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nMin = 5\n",
    "\n",
    "class Node:\n",
    "    name = None\n",
    "    n = 0\n",
    "    ss = {}\n",
    "    child = {}\n",
    "    \n",
    "    def __str__(self):\n",
    "        if self.name is None:\n",
    "            return 'Leaf'\n",
    "        return self.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initDictionary(dic):\n",
    "    for key in dic:\n",
    "        if isinstance(dic[key], int):\n",
    "            dic[key] = 0\n",
    "        else:\n",
    "            initDictionary(dic[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initSufficientStatistics(data):\n",
    "    dic = {}\n",
    "    \n",
    "    for attribute in data.columns:\n",
    "        dic[attribute] = {}\n",
    "    \n",
    "    cc = np.unique(data['class'])\n",
    "    \n",
    "    for attribute in data.columns:\n",
    "        tt = np.unique(data[attribute])\n",
    "        \n",
    "        for val in tt:\n",
    "            dic[attribute][val] = {}\n",
    "            \n",
    "            for val2 in cc:\n",
    "                dic[attribute][val][val2] = 0\n",
    "    \n",
    "    for val2 in cc:\n",
    "        dic['class'][val2] = 0\n",
    "    \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newSufficientStatistics(ss, dropAttribute):\n",
    "    newSS = ss.copy()\n",
    "    #newSS = newSS.pop(dropAttribute)\n",
    "    del newSS[dropAttribute]\n",
    "    return newSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateSufficientStatistics(query, ss):\n",
    "    for key in query:\n",
    "        if key != 'class':\n",
    "            if key in ss:\n",
    "                ss[key][query[key]][query['class']]+=1\n",
    "                ss['class'][query['class']]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNewNode(oldNode):\n",
    "    newNode = Node()\n",
    "    newNode.n = 0\n",
    "    newNode.ss = newSufficientStatistics(oldNode.ss, oldNode.name)\n",
    "    initDictionary(newNode.ss)\n",
    "    newNode.child = {}\n",
    "    return newNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info(dic):\n",
    "    if 'class' in dic:\n",
    "        s = 0\n",
    "        for key in dic['class']:\n",
    "            s += dic['class'][key]\n",
    "        \n",
    "        if s == 0:\n",
    "            return 0,0\n",
    "\n",
    "        entropy = 0\n",
    "        \n",
    "        for key in dic['class']:\n",
    "            if dic['class'][key] != 0:\n",
    "                entropy += ( (-dic['class'][key]/s) * np.log2(dic['class'][key]/s) )\n",
    "        return s,entropy\n",
    "    else:\n",
    "        dd = {}\n",
    "        dd['class'] = {}\n",
    "        \n",
    "        # {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0}\n",
    "        for key in dic:\n",
    "            if key not in dd['class']:\n",
    "                dd['class'][ key ] = 0\n",
    "            dd['class'][ key ] += dic[key]\n",
    "            \n",
    "        return info(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain(dic,feature):\n",
    "    ##Calculate the entropy of the dataset    \n",
    "    s,infoTotal = info(dic)\n",
    "    infoFeature = 0\n",
    "    \n",
    "    attribute = dic[feature]\n",
    "    \n",
    "    for val in attribute:\n",
    "        sI,infoI = info(attribute[val])\n",
    "        infoFeature += (sI/s*infoI)\n",
    "    \n",
    "    #Calculate the information gain of perticular feature or attribute\n",
    "    gainV = infoTotal - infoFeature\n",
    "    return gainV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solveVFDT(query,root):\n",
    "    #print(root.name)\n",
    "    if root.name is not None:\n",
    "        solveVFDT( query,root.child[ query[root.name] ] )\n",
    "    else:\n",
    "        updateSufficientStatistics(query,root.ss)\n",
    "        root.n += 1\n",
    "        \n",
    "        if root.n % nMin == 0:\n",
    "            \n",
    "            cnt = 0\n",
    "            for v in root.ss['class']:\n",
    "                cnt += root.ss['class'][v] == 0\n",
    "\n",
    "            if cnt < ( len( root.ss['class'] ) - 1 ):\n",
    "                \n",
    "                gainAll = {}\n",
    "\n",
    "                for feature in query:\n",
    "                    if feature == 'class':\n",
    "                        continue\n",
    "                    else:\n",
    "                        if feature in root.ss:\n",
    "                            gainAll[feature] = gain(root.ss,feature)\n",
    "                \n",
    "                maxGainFeature1 = max(gainAll, key=gainAll.get)\n",
    "                maxGain1 = gainAll[ maxGainFeature1 ]\n",
    "                gainAll.pop(maxGainFeature1)\n",
    "                \n",
    "                maxGainFeature2 = max(gainAll, key=gainAll.get)\n",
    "                maxGain2 = gainAll[ maxGainFeature2 ]\n",
    "                \n",
    "                R = 0.5\n",
    "                delta = 0.01\n",
    "                epsilon = np.sqrt(R**2 * np.log(1/delta) / (2*root.n))\n",
    "                \n",
    "                #if (maxGain1-maxGain2)>epsilon :\n",
    "                root.name = maxGainFeature1\n",
    "                d = root.ss\n",
    "                \n",
    "                for childd in d[ maxGainFeature1 ]:\n",
    "                    temp = createNewNode(root)\n",
    "                    if temp.ss is not None:\n",
    "                        root.child[ childd ] = createNewNode(root)\n",
    "                    else:\n",
    "                        #root.name = None\n",
    "                        break\n",
    "        #print(root.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VFDT(data,className=\"Survived\"):\n",
    "    data.rename(columns={className: \"class\"},inplace=True)\n",
    "    train,test = train_test_split(data)\n",
    "    if className == 'Survived':\n",
    "        train,test = cleanTitanicData(train,test)\n",
    "    \n",
    "    root = Node()\n",
    "    root.ss = initSufficientStatistics(train)\n",
    "\n",
    "    dic = train.to_dict('index')\n",
    "    for key in dic:\n",
    "        query = dic[key]\n",
    "        solveVFDT(query,root)\n",
    "    printTree(root,0)\n",
    "#     formatData(tree,0)\n",
    "#     print(testModel(test,tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = pd.read_csv('data/train.csv')\n",
    "#VFDT(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTree(root,s):\n",
    "    print(str(\"\\t\"*s)+str(root))\n",
    "    if root.name is not None:\n",
    "        for key in root.child:\n",
    "            printTree( root.child[ key ] , s+1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing tree in visible format\n",
    "\n",
    "def formatData(t,s):\n",
    "    if not isinstance(t,dict) and not isinstance(t,list):\n",
    "        print(str(\"\\t\"*s)+str(t))\n",
    "    else:\n",
    "        for key in t:\n",
    "            print(\"\\t\"*s+str(key))\n",
    "            if not isinstance(t,list):\n",
    "                formatData(t[key],s+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hair\n",
      "\tlegs\n",
      "\t\ttoothed\n",
      "\t\t\tLeaf\n",
      "\t\t\teggs\n",
      "\t\t\t\tLeaf\n",
      "\t\t\t\tLeaf\n",
      "\t\tfeathers\n",
      "\t\t\tLeaf\n",
      "\t\t\teggs\n",
      "\t\t\t\tLeaf\n",
      "\t\t\t\tairbone\n",
      "\t\t\t\t\tLeaf\n",
      "\t\t\t\t\tLeaf\n",
      "\t\tLeaf\n",
      "\t\taquatic\n",
      "\t\t\tLeaf\n",
      "\t\t\tLeaf\n",
      "\t\tLeaf\n",
      "\taquatic\n",
      "\t\tlegs\n",
      "\t\t\tLeaf\n",
      "\t\t\tLeaf\n",
      "\t\t\tmilk\n",
      "\t\t\t\tLeaf\n",
      "\t\t\t\tLeaf\n",
      "\t\t\tLeaf\n",
      "\t\t\tLeaf\n",
      "\t\tLeaf\n"
     ]
    }
   ],
   "source": [
    "#Import the dataset and define the feature as well as the target datasets / columns#\n",
    "dataset = pd.read_csv('data/zoo.csv',\n",
    "                      names=['animal_name','hair','feathers','eggs','milk',\n",
    "                                                   'airbone','aquatic','predator','toothed','backbone',\n",
    "                                                  'breathes','venomous','fins','legs','tail','domestic','catsize','class',])#Import all columns omitting the fist which consists the names of the animals\n",
    "\n",
    "#We drop the animal names since this is not a good feature to split the data on\n",
    "dataset=dataset.drop('animal_name',axis=1)\n",
    "\n",
    "VFDT(dataset,className='class')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
